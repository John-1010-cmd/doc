---
title: MySQL
date: 2023-04-27
updated : 2023-04-27
categories: 
- MySQL
tags: 
- MySQL
description: 这是一篇关于MySQL的长篇Blog，主要介绍了索引、MySQL锁、MySQL日志、MVCC、@Transaction注解、分库分表。
---

## 索引

### 聚集索引和非聚集索引

#### 聚集索引（聚簇索引）

1. 索引内容本身就是目录，不需要再去查其他目录来找到需要找的内容
2. 叶子节点上包含该行的所有信息，找到该叶子节点的时候不需要再回表
3. 每个表只能有一个聚集索引，因为目录只能按照一种方法进行排序

#### 非聚集索引

1. 一张表的聚集索引个数可能有多个，最多可以创建249个非聚集索引
2. 先建聚集索引才能创建非聚集索引
3. 非聚集索引数据与索引不同序
4. 非聚集索引在叶节点上有一个“指针”直接指向要查询的数据区域

#### 聚集索引和非聚集索引的比较

- 查询速度上来说：聚集索引优于非聚集索引
- 插入数据的速度上来说：非聚集索引要比聚集索引要快
- 聚集索引一个表只能有一个，而非聚集索引一个表可以存在多个
- 聚集索引存储记录是物理上连续存在，而非聚集索引是逻辑上的连续，物理存储并不连续
- 聚集索引:物理存储按照索引排序；聚集索引是一种索引组织形式，索引的键值逻辑顺序决定了表数据行的物理存储顺序
- 非聚集索引:物理存储不按照索引排序；非聚集索引则就是普通索引了，仅仅只是对数据列创建相应的索引，不影响整个表的物理存储顺序
- 聚簇索引的叶节点就是数据节点。而非聚簇索引的叶节点仍然是索引节点，只不过有一个指针指向对应的数据块。

#### 唯一索引

唯一索引保证了索引列的值在整个表中是唯一的，因此在建立唯一索引时，如果有重复的值会被拒绝。在建立唯一索引时，可以对单个列或多个列进行索引。唯一索引可以大大提高查询效率，但由于需要检查唯一性约束，因此在插入和更新数据时，唯一索引可能会稍微降低一些性能。

### 联合索引、索引覆盖和索引下推

#### 联合索引和最左匹配原则

联合索引（也叫组合索引、复合索引、多列索引）是指对表上的多个列进行索引。联合索引的创建方法跟单个索引的创建方法一样，不同之处仅在于有多个索引列。

最左匹配原则：在通过联合索引检索数据时，从索引中最左边的列开始，一直向右匹配，如果遇到范围查询(>、<、between、like等)，就停止后边的匹配。

```sql
create index idx_abc on test(a,b,c);
-- 如下会用到索引
select * from test where a = xxx;
select * from test where a = xxx and b = xxx;
select * from test where a = xxx and b = xxx and c = xxx;
select * from test where a like 'xxx%';
select * from test where a > xxx;
select * from test where a = xxx order by b;
select * from test where a = xxx and b = xxx order by c group by a;
-- mysql中的优化器会帮忙调整顺序
select * from test where b = xxx and a = xxx;
select * from test where a = xxx and c = xxx and b = xxx;
-- 如下查询只用到联合索引的一部分
select * from test where a = xxx and c = xxx;       -- 可以用到 a 列的索引，用不到 c 列索引。
select * from test where a like 'xxx%' and b = xxx; -- 可以用到 a 列的索引，用不到 b 列的索引。
select * from test where a > xxx and b = xxx;       -- 可以用到 a 列的索引，用不到 b 列的索引。
-- 如下完全用不到索引
select * from test where b = xxx;
select * from test where c = xxx;
select * from test where a like '%xxx';			-- 不满足最左前缀
select * from test where d = xxx order by a;	-- 出现非排序使用到的索引列 d 
select * from test where a + 1 = xxx;	        -- 使用函数、运算表达式及类型隐式转换等
```

#### 索引覆盖

索引覆盖（Index Covering）是指查询语句可以直接通过索引的叶子节点获取需要的数据，而不需要回到表格中去查询数据。

在一个索引中，每个叶子节点包含了索引列以及指向相应数据行的指针，如果查询语句中要求的列都在索引列中，那么只需要查找索引即可，不需要回到表格中去查询数据，这样可以大大减少查询的时间，提高查询效率。

索引覆盖可以带来很多的好处：

- 辅助索引不包含行数据的所有信息，故其大小远小于聚簇索引，因此可以减少大量的IO操作。
- 索引覆盖只需要扫描一次索引树，不需要回表扫描聚簇索引树，所以性能比回表查询要高。
- 索引中列值是按顺序存储的，索引覆盖能避免范围查询回表带来的大量随机IO操作。 

#### 索引下推

索引下推是把本应该在 server 层进行筛选的条件，下推到存储引擎层来进行筛选判断，这样能有效减少回表。

在传统的查询中，查询引擎会首先扫描索引找到符合条件的行，然后再从磁盘读取相应的数据进行过滤，这个过程会涉及到大量的磁盘I/O操作，如果数据量很大，查询的效率就会非常低下。

而使用索引下推，则可以将查询条件下推到存储引擎层，只读取符合条件的数据行，从而减少了磁盘I/O操作，提高了查询效率。具体实现方式是将查询条件转化为一个WHERE子句，然后将该WHERE子句下推到存储引擎层，在引擎层进行条件过滤，只将符合条件的数据行返回给查询引擎。

```sql
set optimizer_switch='index_condition_pushdown=off'; 	-- 关闭索引下推
set optimizer_switch='index_condition_pushdown=on';		-- 开启索引下推
```

### 索引总结

#### 索引的使用场景

应创建索引的场景

1. 经常需要搜索的列上
2. 作为主键的列上
3. 经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度
4. 经常需要根据范围进行搜索的列上
5. 经常需要排序的列上
6. 经常使用在where子句上面的列上

不应创建索引的场景

1. 查询中很少用到的列
2. 对于那些具有很少数据值的列，比如数据表中的性别列，bit数据类型的列
3. 对于那些定义为text，image的列，因为这些列的数据量相当大
4. 当对修改性能的要求远远大于搜索性能时，因为当增加索引时，会提高搜索性能，但是会降低修改性能

#### 索引失效

1. 尽量使用全值匹配
2. 最左前缀发要遵守
3. 不要在索引上做计算
4. 范围条件右边的列失效
5. 尽量使用覆盖索引
6. 索引字段上不要使用不等
7. 主键索引字段上不可以判断null
8. 索引字段使用like不以通配符开头
9. 索引字段字符串要加单引号
10. 索引字段不要使用or

#### B-tree和B+tree

- B+树内节点不存储数据，所有数据存储在叶节点导致查询时间复杂度固定为log n
- B树查询时间复杂度不固定，与Key在树中的位置有关，最好为O(1)
- B+树叶节点两两相连可大大增加区间访问性，可使用在范围查询等
- B+树更适合外部存储（存储磁盘数据）。由于内节点无data域，每个节点能索引的范围更大更精确。

## MySQL的各种锁

MySQL不同的存储引擎支持不同的锁机制。MySQL大致可归纳为以下三种锁：

- 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。
- 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。
- 页面锁：开销和加锁时间介于表锁和行锁之间；会出现死锁；锁定粒度介于表锁和行锁之间，并发度一般。

### MyIsam

在使用MyIsam时，只可以使用表级锁，而MySQL的表级锁有两种模式：

**表共享锁**（Table Read Lock）和**表独占写锁**（Table Write Lock），工作时表现如下：

- 对某一个表的读操作，不会阻塞其他用户对同一表请求，但会阻塞对同一表的写请求
- 对MyIsam的写操作，则会阻塞其他用户对同一表的读和写操作
- MyIsam表的读操作和写操作之间，以及写操作之间是串行的

当一个线程获得对一个表的写锁后，只有持有锁的线程可以对表进行更新操作。其他线程的读写操作都会等待，知道锁被释放为止。

#### 如何加表锁

MyIsam在执行查询操作（select）前，会自动给涉及的所有表加读锁，在执行更新操作（update, delete, insert等）前，会自动给涉及的表加写锁。这个过程并不需要用户干预，因此用户一般不需要直接用 lock table 命令给MyIsam表显式加锁。

用 lock tables 给表显式加表锁时，必须同时取得所有涉及的表的锁。也就是说，在执行 lock tables 后，只能访问显式加锁的这些表，而不能访问未加锁的表，同时，如果加的是读锁，那么只能执行查询操作，而不能执行更新操作。在自动加锁的情况也基本如此，MySQL会一次性获得SQL语句所需要的全部锁。这也正是MyIsam表不会出现死锁（Deadlock Free）的原因。另外，MySQL支持锁升级，即在条件满足时，允许从表共享锁升级为表独占锁。

一个session使用lock table命令给表加了读锁，这个session可以查询锁定表中的记录，但更新或访问其他表都会提示错误，同时，另外一个session可以查询表中的记录，但更新就会出现锁等待。

当使用lock table时，不仅需要一次锁定用到的所有表，而且同一个表在SQL语句中出现多少次，就要通过与SQL语句中相同的别名锁多少次，否则也会出错。

#### 并发锁

在一定条件下，MyIsam存储引擎有一个系统变量concurrent_insert，专门用以控制其并发插入的行为，其值分别可以为0、1、2。使用concurrent_insert可以在插入大量数据时提高插入性能，但是也会对查询操作造成一定的影响。如果对读取操作的性能要求较高，建议关闭concurrent_insert。

- 0：OFF 关闭并发插入，这是默认的模式
- 1：ON 开启并发插入，但只有在插入语句中没有指定锁的情况下生效
- 2：AUTO 开启并发插入，如果插入语句中没有指定锁则生效，否则相当于OFF

#### MyIsam的锁调度

MyIsam存储引擎的读和写锁是互斥，读操作是串行。那么，一个进程请求某个MyIsam表的读锁，同时另一个进程也请求同一表的写锁，写进程先获得锁，即使读进程先请求先到锁等待队列，写请求后到，写锁也会插到读请求之前。因为MySQL认为写请求一般比读请求重要，这也正是MyIsam表不适合大量更新操作和查询操作应用的原因，因为大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。

- 通过指定启动参数low-priority-updates，使MyIsam引擎默认给读请求优先的权利
- 通过执行命令set low_priority_updates=1，使该连接发出的更新请求优先级降低
- 通过指定insert, update, delete语句的low_priority属性，降低该语句的优先级

以上三种方法要么更新优先，要么查询优先，还有一种折中的方法来调节读写冲突，即给系统参数max_write_lock_count设置一个合适的值，当一个表的读锁达到这个值后，MySQL便暂时将写请求的优先级降低，给读进程一定获得锁的机会。

一些需要长时间运行的查询操作，也会使写进程“饿死”，因此应尽量避免出现长时间运行的查询操作，如果复杂查询不可避免，应尽量安排在数据库空闲时段执行。

### InnoDB

InnoDB与MyIsam的最大不同有两点：

1. 支持事务
2. 采用了行级锁

#### 事务及其ACID属性

- 原子性（Actomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行
- 一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态
- 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的。
- 持久性（Durable）：事务完成后，对于数据的修改是永久性的，即使出现系统故障也能够保持。

#### 并发事务带来的问题

- 更新丢失（Lost update）：两个或多个事务选择同一行，最后的更新覆盖其他事务所做的更新。
- 脏读（Dirty read）：一个事务修改一条记录，事务提交前，另一个书屋读取了这些“脏”的数据。
- 不可重重复读（Non-Repeatable reads）：一个事务在读取某些数据已经发生了改变，或某些记录已经被删除。
- 幻读（Phantom reads）：一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据。

#### InnoDB都有哪些锁

1. 行锁
   1. 共享锁（lock in share mode）
   2. 排他锁（for update)
2. 意向锁（表级别）
   1. 意向共享锁
   2. 意向排他锁
3. 间隙锁
4. Next-key lock：行锁（排他锁） + 间隙锁

#### InnoDB的行锁模式及加锁方法

- 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。xxx lock in share mode
- 排他锁（X）：允许获取排他锁的事务更新数据，阻止其他事务获得相同的数据集的共享读锁和排他写锁。xxx for update
- 意向共享锁（IS）：事务打算给数据行共享锁，事务在给一个数据行加共享锁前必须获得该表的IS锁。
- 意向排他锁（IX）：事务打算给数据行加排他锁，事务在给一个数据行加排他锁前必须获得该表的IX锁。

|      | X    | IX   | S    | IS   |
| ---- | ---- | ---- | ---- | ---- |
| X    | 冲突 | 冲突 | 冲突 | 冲突 |
| IX   | 冲突 | 兼容 | 冲突 | 兼容 |
| S    | 冲突 | 冲突 | 兼容 | 兼容 |
| IS   | 冲突 | 兼容 | 兼容 | 兼容 |

如果一个事务请求的锁模式与当前的锁兼容，InnoDB就请求的锁授予该事务，如果两者不兼容，该事务就要等待锁释放。

意向锁是InnoDB自动加的，不需用户干预。对于update, delete, insert 语句，InnoDB会自动给涉及的数据集加排他锁，对于普通select语句，InnoDB会自动给涉及数据集加共享锁，事务也可以显式给记录集加共享锁或排他锁。

- 共享锁：select * from table_name where ... lock in share mode
- 排他锁：select * from table_name where ... for update

用select ... in share mode获得共享锁，主要用在需要数据依存关系时确认某行记录是否存在，并确保没有人对这个记录进行updaet或delete操作。但如果当前事务也需要对该记录进行更新操作，则很有可能造成死锁，对于锁定行记录后需要进行更新操作的应用，应该使用select ... for update

#### InnoDB行锁实现方式

InnoDB行锁是通过索引上的索引项来实现的，这点与Oracle不同，后者是通过在数据中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才会使用行级锁，否则，InnoDB将使用表锁（如果是RR/Serializable级别，将在主键上使用Next-Key Locks（行锁+间隙锁）来实现锁表的操作）

在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

因此，如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量后放。

#### 间隙锁

用范围条件而不是相等条件检索数据，并请求共享或排他锁（可重复读、串行化级别下才有效）会给符合条件的已有数据的索引项加锁，对于键值在条件范围内但并不存在的记录加锁，这种锁机制就是所谓的间隙锁，通常是一个开区间。

InnoDB使用间隙锁的目的，一方面是为了防止幻读，以满足相关隔离级别的要求，另一方面是为了满足其恢复和负值的需要。

在使用范围条件检索并锁定记录时，InnoDB这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。

间隙锁的存在可能会导致死锁。不同session下的间隙锁之间不会冲突（间隙锁不互锁），跟间隙锁存在冲突关系的是，往这个间隙中插入一个记录，这个操作。

#### Next-Key锁

next-key lock是InnoDB加锁的基本单位，他是一个前开后闭的区间，即行锁+间隙锁。

#### InnoDB加锁规则

- 原则1：加锁的基本单位是next-key lock
- 原则2：查找过程中访问到的对象才会加锁
- 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁
- 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁
- 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止

#### 什么时候使用表锁

对于InnoDB表，绝大部分情况下都应该使用行级锁，因为事务和行锁往往是我们选择InnoDB表的理由。但个别特殊事务中也可以考虑使用表级锁。

1. 事务需要更新大部分或全部数据，表又较大，如果使用默认的行锁，不仅这个事务执行效率低，而且可能造成其他事务长时间等待和锁冲突，这种情况下可以考虑使用表锁来提高该事务的执行速度。
2. 事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。这种情况也可以考虑一次性锁定事务涉及的表，从而避免死锁、减少数据库因事务回滚带来的开销。

当然，应用中这两种事务不能太多，否则，就应该考虑使用MyIsam表。

在InnoDB下使用表锁要注意两点：

1. 使用LOCK TALBES虽然可以给InnoDB加表级锁，但必须说明的是，表锁不是由InnoDB存储引擎层管理的，而是由其上一层ＭySQL Server负责的，仅当autocommit=0、innodb_table_lock=1（默认设置）时，InnoDB层才能知道MySQL加的表锁，ＭySQL Server才能感知InnoDB加的行锁，这种情况下，InnoDB才能自动识别涉及表级锁的死锁；否则，InnoDB将无法自动检测并处理这种死锁。
2. 在用LOCAK TABLES对InnoDB锁时要注意，要将AUTOCOMMIT设为0，否则ＭySQL不会给表加锁；事务结束前，不要用UNLOCAK TABLES释放表锁，因为UNLOCK TABLES会隐含地提交事务；而COMMIT或ROLLBACK并不能释放用LOCAK TABLES加的表级锁，所以一般我们必须先提交事务后，再用UNLOCK TABLES释放表锁。

#### 关于死锁

MyIsam表锁是deadlock free的，因为MyIsam总是一次性获得所需的全部锁，要么全部满足要么等待，因此不会出现死锁。但在InnoDB中，除单个SQL组成的事务外，锁是逐步获得的，这就决定了InnoDB发生死锁是可能的。

发生死锁后，InnoDB一般都能自动检测到，并使一个事务释放锁并退回，另一个事务获得锁，继续完成事务。有以下两种处理方式：

1. 直接进入等待，直到超时，超时时间可以通过参数innodb_lock_wait_timeout来设置，默认50s
2. 主动死锁检测，发现死锁后，主动回滚锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。

通常来说，死锁都是应用设计的问题，通过调整业务流程、数据库对象设计、事务大小、以及访问数据库的SQL语句，绝大部分都可以避免。以下是避免死锁的常用方法：

1. 在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序为访问表，这样可以大大降低产生死锁的机会。
2. 在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低死锁的可能。
3. 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应该先申请共享锁，更新时再申请排他锁，甚至死锁。
4. 在REPEATEABLE-READ隔离级别下，如果两个线程同时对相同条件记录用SELECT...FOR UPDATE加排他锁，在没有符合该记录情况下，两个线程都会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成READ COMMITTED，就可以避免问题。
5. 当隔离级别为READ COMMITED时，如果两个线程都先执行SELECT...FOR UPDATE，判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第１个线程提交后，第２个线程会因主键冲突出错，但虽然这个线程出错了，却会获得一个排他锁！这时如果有第３个线程又来申请排他锁，也会出现死锁。对于这种情况，可以直接做插入操作，然后再捕获主键冲突异常，或者在遇到主键重错误时，总是执行ROLLBACK释放获得的排他锁。

 如果出现死锁，可以用SHOW ENGINE INNODB STATUS命令来确定最后一个死锁产生的原因和改进措施。

### 总结

对于MyIsam的表锁，主要有以下几点：

1. 共享读锁（S）之间是兼容的，但共享读锁（S）和排他写锁（X）之间，以及排他写锁之间（X）是互斥的，也就是说读和写是串行的。
2. 在一定条件下，ＭyISAM允许查询和插入并发执行，我们可以利用这一点来解决应用中对同一表和插入的锁争用问题。
3. ＭyISAM默认的锁调度机制是写优先，这并不一定适合所有应用，用户可以通过设置LOW_PRIPORITY_UPDATES参数，或在INSERT、UPDATE、DELETE语句中指定LOW_PRIORITY选项来调节读写锁的争用。
4. 由于表锁的锁定粒度大，读写之间又是串行的，因此，如果更新操作较多，ＭyISAM表可能会出现严重的锁等待，可以考虑采用InnoDB表来减少锁冲突。

对于InnoDB表，主要有以下几点：

1. InnoDB的行锁是基于索引实现的，如果不通过索引访问数据，InnoDB会使用表锁。
2. InnoDB间隙锁机制，以及InnoDB使用间隙锁的原因。
3. 在不同的隔离级别下，InnoDB的锁机制和一致性读策略不同。
4. ＭySQL的恢复和复制对InnoDB锁机制和一致性读策略也有较大影响。
5. 锁冲突甚至死锁很难完全避免。

在了解InnoDB的锁特性后，用户可以通过设计和SQL调整等措施减少锁冲突和死锁，包括：

1. 尽量使用较低的隔离级别
2. 精心设计索引，并尽量使用索引访问数据，使加锁更精确，从而减少锁冲突的机会。
3. 选择合理的事务大小，小事务发生锁冲突的几率也更小。
4. 给记录集显示加锁时，最好一次性请求足够级别的锁。比如要修改数据的话，最好直接申请排他锁，而不是先申请共享锁，修改时再请求排他锁，这样容易产生死锁。
5. 不同的程序访问一组表时，应尽量约定以相同的顺序访问各表，对一个表而言，尽可能以固定的顺序存取表中的行。这样可以大减少死锁的机会。
6. 尽量用相等条件访问数据，这样可以避免间隙锁对并发插入的影响。
7. 不要申请超过实际需要的锁级别；除非必须，查询时不要显示加锁。
8. 对于一些特定的事务，可以使用表锁来提高处理速度或减少死锁的可能。

## bin log, redo log 以及undo log

 ### bin log

#### 定义

bin log应该说是Mysql里**最核心**的日志，是MySQL数据库级别的文件，记录对MySQL数据库各种引擎下执行修改的所有操作（包括DDL和DML语句），不会记录select和show语句,主要用于恢复数据库和同步数据库（主从同步）。

#### bin log的三种日志格式

bin log有三种记录格式，分别是ROW、STATEMENT、MIXED。

1. ROW： 基于变更的数据行进行记录，如果一个update语句修改一百行数据，那么这种模式下就会记录100行对应的记录日志。
2. STATEMENT（mysql默认的日志格式）：基于SQL语句级别的记录日志，相对于ROW模式，STATEMENT模式下只会记录这个update 的语句。所以此模式下会非常节省日志空间，也避免着大量的IO操作。
3. MIXED：混合模式，此模式是ROW模式和STATEMENT模式的混合体，一般的语句修改使用statment格式保存binlog，如一些函数的日志；对于statement无法完成主从复制的操作，则采用row格式保存binlog。

使用 row 格式的 binlog 时，在进行数据同步或恢复的时候不一致的问题更容易被发现，因为它是基于数据行记录的。而使用 mixed 或者 statement 格式的 binlog 时，很多事务操作都是基于SQL逻辑记录，我们都知道一个SQL在不同的时间点执行它们产生的数据变化和影响是不一样的，所以这种情况下，数据同步或恢复的时候就容易出现不一致的情况。

#### bin log 的刷盘时机

bin log 写入策略在进行事务的过程中，首先会把bin log 写入到bin log cache中（因为写入到cache中会比较快，一个事务通常会有多个操作，避免每个操作都直接写磁盘导致性能降低），事务最终提交的时候再吧binlog 写入到磁盘中。当然事务在最终commit的时候bin log是否马上写入到磁盘中是由参数 sync_binlog 配置来决定的。

1. sync_binlog=0 的时候，表示每次提交事务bin log不会马上写入到磁盘，而是先写到page cache,相对于磁盘写入来说写page cache要快得多,不过在Mysql 崩溃的时候会有丢失日志的风险。
2. sync_binlog=1 的时候，表示每次提交事务都会执行 fsync 写入到磁盘（mysql默认的机制）；
3. sync_binlog的值大于1 的时候，表示每次提交事务都 先写到page cache，只有等到积累了N个事务之后才fsync 写入到磁盘，同样在此设置下Mysql 崩溃的时候会有丢失N个事务日志的风险。很显然三种模式下，sync_binlog=1 是强一致的选择，选择0或者N的情况下在极端情况下就会有丢失日志的风险，具体选择什么模式还是得看系统对于一致性的要求。

### redo log

#### 定义

redo log是innoDB引擎级别，用来记录innoDB存储引擎的事务日志，不管事务是否提交都会记录下来，用于数据恢复。当数据库发生故障，innoDB存储引擎会使用redo log恢复到发生故障前的时刻，以此来保证数据的完整性。将参数innodb_flush_log_at_tx_commit设置为1，那么在执行commit时会将redo log同步写到磁盘。

redo log 的设计目标是支持innodb的“事务”的特性，事务ACID特性分别是原子性、一致性、隔离性、持久性， 一致性是事务的最终追求的目标，隔离性、原子性、持久性是达成一致性目标的手段，隔离性是通过锁机制来实现的。 而事务的原子性和持久性则是分别通过undo log和redo log 来保障的。

#### bin log可以替换redo log用于宕机后的数据恢复吗？

1. 最核心的一点就是redo log记录的数据变更粒度和binlog的数据变更粒度是不一样的，也正因为这个binlog是没有进行崩溃恢复事务数据的能力的。
2. bin log 是以表为记录主体，在ROW模式下，bin log保存的表的每行变更记录。redo log则是记录着磁盘数据的变更日志，以磁盘的最小单位“页”来进行记录。
3. 不知道 bin log中的sql语句是否执行成功，因为没有事务是否提交成功的状态记录字段

#### redo log刷盘时机

1. redo lo占用的空间是一定的，并不会无线增大（可以通过参数设置），写入的时候是进顺序写的，所以写入的性能比较高。当redo log空间满了之后又会从头开始以循环的方式进行覆盖式的写入。
2. 在写入redo log的时候也有一个redo log buffer，日志什么时候会刷到磁盘是通过innodb_flush_log_at_trx_commit 参数决定。
   - innodb_flush_log_at_trx_commit=0 ，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中，只有当buffer中存储了多个事务日志才会刷入磁盘中;
   - innodb_flush_log_at_trx_commit=1，表示每次事务提交时都将 redo log 直接持久化到磁盘；
   - innodb_flush_log_at_trx_commit=2，表示每次事务提交时都只是把 redo log 写到 page cache。
3. 还有其它两种情况会把redo log buffer中的日志刷到磁盘。
   - 定时处理：有线程会定时(每隔 1 秒)把redo log buffer中的数据刷盘。
   - 根据空间处理：redo log buffer 占用到了一定程度( innodb_log_buffer_size 设置的值一半)占，这个时候也会把redo log buffer中的数据刷盘。

### undo log

除了记录redo log外，当进行数据修改时还会记录undo log，undo log用于数据的撤回操作，它保留了记录修改前的内容。通过undo log可以实现事务回滚，并且可以根据undo log回溯到某个特定的版本的数据，实现MVCC。

### bin log和redo log区别

1. bin log会记录所有日志记录，包括InnoDB、MyISAM等存储引擎的日志；redo log只记录innoDB自身的事务日志。
2. bin log主要用于数据库的 主从复制 以及数据恢复工作(比如恢复到数据库的某一个历史版本)，redo log 主要用于发生故障后，让所有数据恢复到发生故障之前的状态。
3. bin log是逻辑日志，记录的是SQL语句；redo log是物理日志，记录的数据格式是 “在某个数据页上做了什么修改”。
4. bin log的日志空间是二进制流式的缓冲区，写完一个缓冲区后，不会覆盖之前的内容，而是重开一个缓冲区，继续写入日志；而redo log的日志空间大小是固定的，通过循环写入的方式将日志写入磁盘，一旦没有空闲空间，则会采用覆盖的方式，覆盖的之前的内容。
5. bin log只在事务提交前被写入一次磁盘(MySQL 5.7.7之后版本的默认保存方式)，一个事务只写一次；而对于redo log，在事务开始时会写入一次磁盘，提交redo log为prepare状态，事务提交后又会写入一次磁盘，提交redo log 为commit状态。

### redo log和undo log区别

目的不同：redo log主要是用于发生故障时的数据恢复操作，将数据恢复到故障发生前的状态，而undo log主要用于事务的回滚，将数据恢复为事务发生前的状态

### redo log, undo log, bin log的生成流程与崩溃恢复

当执行update user_info set name =“李四” where id=1 的时候大致流程如下：

1. 从磁盘读取到id=1的记录，放到内存。
2. 记录undo log 日志。
3. 记录redo log (prepare状态)。
4. 修改内存中的记录。
5. 记录bin log。
6. 提交事务，写入redo log (commit状态)。

如果在上面的某一个阶段数据库崩溃，如何恢复数据。

1. 在第一步、第二步、第三步执行时据库崩溃：因为这个时候数据还没有发生任何变化，所以没有任何影响，不需要做任何操作。
2. 在第四步修改内存中的记录时数据库崩溃：因为此时事务没有commit，所以不管有没有将修改的数据写回磁盘，这里都要进行数据回滚，所以这里会通过undo log进行数据回滚。
3. 第五步写入bin log时数据库崩溃：这里和第四步一样的逻辑，此时事务没有commit，所以这里要进行数据回滚，会通过undo log进行数据回滚。
4. 执行第六步事务提交时数据库崩溃：如果数据库在这个阶段崩溃，那其实事务还是没有提交成功，但是这里并不能像之前一样对数据进行回滚，因为在提交事务前,bin log可能成功写入磁盘了，所以这里要根据两种情况来做决定。

- 如果bin log存在事务记录：那么就"认为"事务已经提交了，这里可以根据redo log对数据进行重做。其实你应该有疑问，其实这个阶段发生崩溃了，最终的事务是没提交成功的,这里应该对数据进行回滚。 这里主要的一个考虑是因为bin log已经成功写入了，而bin log写入后，那么依赖于binlog的其它扩展业务（比如：从库已经同步了日志进行数据的变更）数据就已经产生了，如果这里进行数据回滚，那么势必就会造成主从数据的不一致。
- 如果bin log不存在事务记录，那么这种情况事务还未提交成功，所以会对数据进行回滚。

## MVCC

MVCC，多版本并发控制，它是一种并发控制方法，一般在数据库管理系统中，实现数据库的并发访问，在编程语言中实现事务内存。主要为了提高并发性能。

### 为什么需要MVCC

1. 数据库原生的锁
   最原生的锁，锁住一个资源后会禁止其他任何线程访问同一个资源。但是很多应用的一个特点都是读多写少的场景，很多数据的读取次数远大于修改的次数，而读取数据间互相排斥显得不是很必要。
2. 读写锁的出现
   读锁和读锁之间不互斥，而写锁和写锁、读锁都互斥。这样就很大提升了系统的并发能力。之后人们发现并发度还是不够。
3. 让读写之间也不冲突的方法，就是读取数据时通过一种类似快照的方式将数据保存下来，这样读锁就和写锁不冲突了，不同的事务session会看到自己特定版本的数据。当然快照是一种概念模型，不同的数据库可能用不同的方式来实现这种功能。

总结：MVCC 就是因为不满意只让数据库采用悲观锁这样性能不佳的形式去解决读写冲突问题，而提出的解决方案，所以在数据库中，因为有了 MVCC，所以我们可以形成两个组合：

- MVCC+悲观锁：MVCC解决读写冲突，悲观锁解决写写冲突
- MVCC+乐观锁：MVCC解决读写冲突，乐观锁解决写写冲突

### MVCC适用于的事务隔离级别

MVCC只在 READ COMMITTED (读取已提交) 和 REPEATABLE READ (可重复读) 两个隔离级别下工作。其他两个隔离级别够和MVCC不兼容, 因为 READ UNCOMMITTED (读取未提交) 总是读取最新的数据行, 而不是符合当前事务版本的数据行。而 SERIALIZABLE (可串行化) 则会对所有读取的行都加锁。

### MVCC实现原理

MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的 3个隐式字段，undo日志 ，Read View 来实现的。

#### 3个隐式字段

- DB_TRX_ID：事务ID，每处理一次加一
- DB_ROLL_PTR：指向undo log的一个指针
- DB_ROW_ID：用于存在聚集索引中的ID

#### undo日志

- insert undo log
  代表事务在 insert 新记录时产生的 undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃
- update undo log事务在进行 update 或 delete 时产生的 undo log ; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被 purge 线程统一清除。

在不考虑redo log 的情况下利用undo log工作的简化过程为：

1. 开始事务
2. 记录数据行数据快照到undo log
3. 更新数据
4. 将undo log写到磁盘
5. 将数据写到磁盘
6. 提交事务

#### Read View

在 MySQL 中，每个事务都有一个 Read View，用于记录该事务开始时间的快照信息。Read View 中记录了所有活跃事务的版本信息，包括该事务本身以及其他事务。在查询时，MySQL 会根据 Read View 来判断每个数据行对该事务是否可见。

## @Transactional注解

### 事务

#### 编程式事务

指在代码中手动的管理事务的提交、回滚等操作，代码侵入性比较强。编程式事务方式需要开发者在代码中手动的管理事务的开启、提交、回滚等操作。

#### 声明式事务

基于AOP面向切面的，它将具体业务与事务处理部分解耦，代码侵入性很低，所以在实际开发中声明式事务用的比较多。声明式事务也有两种实现方式，一是基于TX和AOP的xml配置文件方式，二种就是基于@Transactional注解了。

声明式事务管理方法允许开发者配置的帮助下来管理事务，而不需要依赖底层API进行硬编码。开发者可以只使用注解或基于配置的 XML 来管理事务。

声明式事务对代码没有侵入性，方法内只需要写业务逻辑就可以了，帮助我们节省了很多代码，他会自动帮我们进行事务的开启、提交以及回滚等操作，把程序员从事务管理中解放出来。可以说优点很明显，但是这种方式的缺点也很明显。

1. **声明式事务的粒度问题。**他的最小粒度要作用在方法上。如果想要给一部分代码块增加事务的话，那就需要把这个部分代码块单独独立出来作为一个方法。在大事务耗时过长需要高并发优化的情况下不建议使用声明式事务，建议采用上面提到的编程式事务方式。
2. **声明式事务容易被开发者忽略。**如果开发者没有注意到一个方法是被事务嵌套的，那么就可能会再方法中加入一些如RPC远程调用、消息发送、缓存更新、文件写入等操作。这些操作如果被包在事务中，一是由于这些操作自身是无法回滚的，这就会导致数据的不一致。可能RPC调用成功了，但是本地事务回滚了，可是PRC调用无法回滚了(这里不讨论分布式事务)。二是会拉长整个事务。那么久会导致本事务的数据库连接一直被占用，那么如果类似操作过多，就会导致数据库连接池耗尽。
3. **声明式事务用不对在某些场景下容易失效。**

### @Transactional介绍

#### 可以作用在接口、类、类方法

- 作用于类：当把@Transactional 注解放在类上时，表示所有该类的public方法都配置相同的事务属性信息。
- 作用于方法：当类配置了@Transactional，方法也配置了@Transactional，方法的事务会覆盖类的事务配置信息。
- 作用于接口：不推荐这种使用方法，因为一旦标注在Interface上并且配置了Spring AOP 使用CGLib动态代理，将会导致@Transactional注解失效。

#### @Transactional属性

1. value：String：可选的限定描述符，指定使用的事务管理器
2. propagation：enum: Propagation：可选的事务传播行为设置
   1. Propagation.REQUIRED：如果当前存在事务，则加入该事务，如果当前不存在事务，则创建一个新的事务。( 也就是说如果A方法和B方法都添加了注解，在默认传播模式下，A方法内部调用B方法，会把两个方法的事务合并为一个事务 ）。
   2. Propagation.SUPPORTS：如果当前存在事务，则加入该事务；如果当前不存在事务，则以非事务的方式继续运行。
   3. Propagation.MANDATORY：如果当前存在事务，则加入该事务；如果当前不存在事务，则抛出异常。
   4. Propagation.REQUIRES_NEW：重新创建一个新的事务，如果当前存在事务，暂停当前的事务。( 当类A中的 a 方法用默认Propagation.REQUIRED模式，类B中的 b方法加上采用 Propagation.REQUIRES_NEW模式，然后在 a 方法中调用 b方法操作数据库，然而 a方法抛出异常后，b方法并没有进行回滚，因为Propagation.REQUIRES_NEW会暂停 a方法的事务 )。
   5. Propagation.NOT_SUPPORTED：以非事务的方式运行，如果当前存在事务，暂停当前的事务。
   6. Propagation.NEVER：以非事务的方式运行，如果当前存在事务，则抛出异常。
   7. Propagation.NESTED：和 Propagation.REQUIRED 效果一样。
3. isolation：enum: Isolation：可选的事务隔离级别设置。
   1. Isolation.DEFAULT：使用底层数据库默认的隔离级别。
   2. Isolation.READ_UNCOMMITTED
   3. Isolation.READ_COMMITTED
   4. Isolation.REPEATABLE_READ
   5. Isolation.SERIALIZABLE
4. readOnly：boolean：读写或只读事务，默认读写。
5. timeout：int (in seconds granularity)：事务超时时间设置，默认值为 -1。如果超过该时间限制但事务还没有完成，则自动回滚事务。
6. rollbackFor：Class对象数组，必须继承自Throwable：用于指定能够触发事务回滚的异常类型，可以指定多个异常类型。
7. rollbackForClassName：类名数组，必须继承自Throwable：导致事务回滚的异常类名字数组。
8. noRollbackFor：Class对象数组，必须继承自Throwable：抛出指定的异常类型，不回滚事务，也可以指定多个异常类型。
9. noRollbackForClassName：类名数组，必须继承自Throwable：不会导致事务回滚的异常类名字数组。

#### @Transactional失效场景

1. @Transactional 应用在非 public 修饰的方法上

2. @Transactional 注解属性 propagation 设置错误
   若是错误的配置以下三种propagation，事务将不会回滚

   - PROPAGATION_SUPPORTS

   - PROPAGATION_NOT_SUPPORTED
   - PROPAGATION_NEVER

3. @Transactional 注解属性 rollbackFor 设置错误
   rollbackFor 可以指定能够触发事务回滚的异常类型。Spring默认抛出了未检查unchecked异常（继承自 RuntimeException的异常）或者 Error才回滚事务；其他异常不会触发回滚事务。如果在事务中抛出其他类型的异常，但却期望 Spring 能够回滚事务，就需要指定 rollbackFor属性。

4. 同一个类中方法调用，导致@Transactional失效
   开发中避免不了会对同一个类里面的方法调用，比如有一个类Test，它的一个方法A，A再调用本类的方法B（不论方法B是用public还是private修饰），但方法A没有声明注解事务，而B方法有。则外部调用方法A之后，方法B的事务是不会起作用的。

5. 异常被 catch“吃了”导致@Transactional失效
   如果B方法内部抛了异常，而A方法此时try catch了B方法的异常，那这个事务不能正常回滚。

6. 数据库引擎不支持事务
   MySQL数据库默认使用支持事务的innodb引擎。一旦数据库引擎切换成不支持事务的myisam，那事务就从根本上失效了。

## 分库分表

### 数据切分

数据的切分（Sharding）依据其切分规则的类型。能够分为两种切分模式：垂直切分和水平切分。

一种是依照不同的表（或者Schema）来切分到不同的数据库（主机）之上，这样的切能够称之为数据的垂直（纵向）切分。另外一种则是依据表中的数据的逻辑关系，将同一个表中的数据依照某种条件拆分到多台数据库（主机）上面。这样的切分称之为数据的水平（横向）切分。

垂直切分的最大特点就是规则简单，实施也更为方便，尤其适合各业务之间的耦合度非常低。相互影响非常小，业务逻辑非常清晰的系统。在这样的系统中，能够非常easy做到将不同业务模块所使用的表分拆到不同的数据库中。依据不同的表来进行拆分。对应用程序的影响也更小，拆分规则也会比較简单清晰。

水平切分于垂直切分相比。相对来说略微复杂一些。由于要将同一个表中的不同数据拆分到不同的数据库中，对于应用程序来说，拆分规则本身就较依据表名来拆分更为复杂，后期的数据维护也会更为复杂一些。

当我们某个（或者某些）表的数据量和访问量特别的大，通过垂直切分将其放在独立的设备上后仍然无法满足性能要求，这时候我们就必须将垂直切分和水平切分相结合。先垂直切分，然后再水平切分。才干解决这样的超大型表的性能问题。

### 垂直切分

#### 垂直分库

根据业务耦合性，将关联度低的不同表存储在不同的数据库。做法与大系统拆分为多个小系统类似，按业务分类进行独立划分。与"微服务治理"的做法相似，每个微服务使用单独的一个数据库。

#### 垂直分表

把一个表的多个字段分别拆成多个表，一般按字段的冷热拆分，热字段一个表，冷字段一个表。从而提升了数据库性能

#### 垂直切分优缺点

优点：

- 解决业务系统层面的耦合，业务清晰
- 对不同业务的数据进行分级管理、维护、监控、扩展等
- 高并发场景下，垂直切分一定程度的提升IO、数据库连接数、单机硬件资源的瓶颈

缺点：

- 分库后无法Join，只能通过接口聚合方式解决，提升了开发的复杂度
-  分库后分布式事务处理复杂
- 依然存在单表数据量过大的问题（需要水平切分）

### 水平切分

#### 水平分库

随着业务的增加出现QPS过高，数据库响应速度来不及,一般mysql单机也就1000左右的QPS，如果超过1000就要考虑分库。

#### 水平分表

如果表数据超过1千万，并且还在不断增加数据，那就可以考虑分表。

#### 水平切分优缺点

优点：

- 不存在单库数据量过大、高并发的性能瓶颈，提升系统稳定性和负载能力
- 应用端改造较小，不需要拆分业务模块

缺点：

- 跨分片的事务一致性难以保证
- 跨库的Join关联查询性能较差
- 数据多次扩展难度和维护量极大

### 垂直与水平切分的联合使用

联合切分的优点

- 能够充分利用垂直切分和水平切分各自的优势而避免各自的缺陷。
- 让系统扩展性得到最大化提升。

联合切分的缺点

-  数据库系统架构比較复杂。维护难度更大。
- 应用程序架构也相对更复杂。

### 数据分片规则

1. Hash取模分表

   一般采用Hash取模的切分方式。
   优点

   - 数据分片相对比较均匀，不容易出现热点和并发访问的瓶颈。

   缺点

   - 后期分片集群扩容时，需要迁移旧的数据很难。

   - 容易面临跨分片查询的复杂问题。

2. 数值Range分表
   按照时间区间或ID区间来切分。
   优点

   - 单表大小可控
   - 天然便于水平扩展，后期如果想对整个分片集群扩容时，只需要添加节点即可，无需对其他分片的数据进行迁移 - 使用分片字段进行范围查找时，连续分片可快速定位分片进行快速查询，有效避免跨分片查询的问题。

   缺点

   - 热点数据成为性能瓶颈。
   
3. 一致性Hash算法分表
   一致性Hash算法能很好的解决因为Hash取模而产生的分片集群扩容时，需要迁移旧的数据的难题。

#### 分库分表带来的问题

1. 分布式事务问题

2. 跨节点关联查询Join问题
   切分之前，我们可以通过Join来完成。而切分之后，数据可能分布在不同的节点上，此时Join带来的问题就比较麻烦了，考虑到性能，尽量避免使用Join查询。
   解决方法：

   1. 全局表
      全局表，也可看做是 "数据字典表"，就是系统中所有模块都可能依赖的一些表，为了避免跨库Join查询，可以将 这类表在每个数据库中都保存一份。这些数据通常很少会进行修改，所以也不担心一致性的问题。
   2. 字段冗余
      利用空间换时间，为了性能而避免join查询。例：订单表保存userId时候,也将userName冗余保存一份，这样查询订单详情时就不需要再去查询"买家user表"了。
   3. 数据组装
      在系统层面，分两次查询。第一次查询的结果集中找出关联数据id，然后根据id发起第二次请求得到关联数据。最后将获得到的数据进行字段拼装。

3. 跨节点分页、排序、序数问题
   跨节点多库进行查询时，会出现Limit分页、Order by排序等问题。分页需要按照指定字段进行排序，当排序字段就是分片字段时，通过分片规则就比较容易定位到指定的分片；

   当排序字段非分片字段时，就变得比较复杂了。需要先在不同的分片节点中将数据进行排序并返回，然后将不同分片返回的结果集进行汇总和再次排序，最终返回给用户。

4. 全局主键避重问题
   如果都用主键自增肯定不合理，如果用UUID那么无法做到根据主键排序，所以我们可以考虑通过雪花ID来作为数据库的主键。

5. 数据迁移问题
   采用双写的方式，修改代码，所有涉及到分库分表的表的增、删、改的代码，都要对新库进行增删改。同时，再有一个数据抽取服务，不断地从老库抽数据，往新库写，边写边按时间比较数据是不是最新的。

### 数据切分及整合方案

1. 在每一个应用程序模块中配置管理自己须要的一个（或者多个）数据源。直接訪问各个数据库，在模块内完毕数据的整合（不太现实，因为现在主流的DB架构都是独立一层）；
2. 通过中间代理层来统一管理全部的数据源。后端数据库集群对前端应用程序透明；
